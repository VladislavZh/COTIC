_target_: src.models.base_model.BaseEventModule

net:
  _target_: src.models.components.cotic.cotic.COTIC
  in_channels: 256
  kernel_size: 7
  nb_filters: 512
  nb_layers: 7 # 7
  num_types: ${num_types}
  dropout: 0.1
joined_head:
  _target_: src.models.components.cotic.head.joined_head.ProbabilisticJoinedHead # JoinedHead
  intensity_head:
    _target_: src.models.components.cotic.head.intensity_head.IntensityHeadLinear
    kernel_size: 1
    nb_filters: ${model.net.nb_filters}
    num_types: ${num_types}
  downstream_head:
    _target_: src.models.components.cotic.head.downstream_head.ProbabilisticDownstreamHead # DownstreamHeadSklearnLinear #DownstreamHeadLinear
    compute_every_n_epochs: 10
#    nb_filters: ${model.net.nb_filters} # DownstreamHeadSklearnLinear, DownstreamHeadLinear
#    num_types: ${num_types} # DownstreamHeadSklearnLinear, DownstreamHeadLinear
##    type_loss_coeff: 1 # DownstreamHeadLinear
##    time_loss_coeff: 10 # DownstreamHeadLinear
##    reductions: 2 # DownstreamHeadLinear
##      type: mean # DownstreamHeadLinear
##      time: mean # DownstreamHeadLinear
  uniform_sample_size: 20
optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 1e-3
  weight_decay: 1e-6
init_lr: ${model.optimizer.lr}
scheduler:
  _target_: torch.optim.lr_scheduler.CyclicLR
  _partial_: true
  base_lr: 1e-5
  max_lr: 1e-3
  step_size_up: 200
#  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#  _partial_: true
#  factor: 0.5
#  patience: 10
#  min_lr: 1e-6
warmup_steps: 1500
scheduler_monitoring_params:
  monitor: val/loss
  mode: min
  verbose: true
  interval: epoch
